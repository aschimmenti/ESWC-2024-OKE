{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def process_jsonl_sentences(jsonl_file_path):\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            text_to_infer = json_obj.get('text', '')\n",
    "            print(text_to_infer)\n",
    "process_jsonl_sentences('output.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from chained_classification.run_pipeline import run_pipeline as NERandREL\n",
    "from class_recognition.class_recognition_pipeline import PipelineRunner as DOLCEAligner\n",
    "\n",
    "def infer_years_for_dates(text):\n",
    "    # Pattern to match dates, capturing the date part and the optional year\n",
    "    date_pattern = re.compile(r'(\\d{1,2}(?:st|nd|rd|th)?\\s(?:January|February|March|April|May|June|July|August|September|October|November|December))(?:\\s(\\d{4}))?', re.I)\n",
    "    \n",
    "    # Find all dates in the text\n",
    "    dates = [(match.group(0), match.start(), match.end(), match.group(2)) for match in date_pattern.finditer(text)]\n",
    "    \n",
    "    # If no dates found, return the original text\n",
    "    if not dates:\n",
    "        return text\n",
    "    \n",
    "    # Process dates to infer missing years\n",
    "    for i, (_, start, end, year) in enumerate(dates):\n",
    "        if year is None:\n",
    "            # Look for the nearest date with a year, searching both directions\n",
    "            prev_years = [prev_year for _, _, _, prev_year in dates[:i] if prev_year is not None]\n",
    "            next_years = [next_year for _, _, _, next_year in dates[i+1:] if next_year is not None]\n",
    "            \n",
    "            # Determine the closest year from previous or next dates\n",
    "            closest_year = prev_years[-1] if prev_years else (next_years[0] if next_years else None)\n",
    "            \n",
    "            # If a closest year is found, replace the date without year with the inferred year\n",
    "            if closest_year:\n",
    "                text = text[:start] + text[start:end] + f\" {closest_year}\" + text[end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def prepare_text_for_DOLCE_aligner(e, text):\n",
    "    return f\"{e.text} in the context of this sentence '{text}'\"\n",
    "\n",
    "def serialize_doc_with_relations(doc):\n",
    "    # Convert the Doc to a basic JSON structure\n",
    "    doc_json = doc.to_json()\n",
    "    doc_json['relations'] = []\n",
    "    doc_json['classes'] = {}  \n",
    "    # Check for and add relation data if present\n",
    "    if hasattr(doc._, 'rel'):\n",
    "        relations = []\n",
    "        for rel in doc._.rel:\n",
    "            # Serialize all relations without filtering based on entity or class\n",
    "            dep_entity = doc.ents[rel.dep]\n",
    "            dest_entity = doc.ents[rel.dest]\n",
    "            serialized_rel = {\n",
    "                \"dep_text\": dep_entity.text,  # Dependent entity text\n",
    "                \"dep\": rel.dep,  # Dependent entity index\n",
    "                \"rel\": rel.relation,  # Relation type\n",
    "                \"dest_text\": dest_entity.text,  # Destination entity text\n",
    "                \"dest\": rel.dest  # Destination entity index\n",
    "            }\n",
    "            relations.append(serialized_rel)\n",
    "        \n",
    "        # Add the serialized relations to the doc_json\n",
    "        doc_json['relations'] = relations\n",
    "\n",
    "    return doc_json\n",
    "\n",
    "def trim_context(entity, context, percentage):\n",
    "    # Normalize spaces in entity and context\n",
    "    entity = \" \".join(entity.split())\n",
    "    context = \" \".join(context.split())\n",
    "    \n",
    "    # Use regex to find the entity in the context with case-insensitive search\n",
    "    match = re.search(re.escape(entity), context, re.IGNORECASE)\n",
    "    if not match:\n",
    "        return context  # Or handle this case as you see fit\n",
    "    \n",
    "    # Extract the start index of the matched entity\n",
    "    start_index = match.start()\n",
    "    entity_words = entity.split()\n",
    "    \n",
    "    # Convert the context into words after finding the match to ensure alignment with entity position\n",
    "    words = context.split()\n",
    "    \n",
    "    # Calculate the position of the entity in terms of word count, not characters\n",
    "    word_count_before_entity = len(re.findall(r'\\S+', context[:start_index]))\n",
    "    \n",
    "    # Calculate the number of words to include around the entity\n",
    "    total_words = len(words)\n",
    "    words_to_include = round(total_words * (percentage / 100))\n",
    "    \n",
    "    # Determine the slice of words to include around the entity\n",
    "    half_words_to_include = words_to_include // 2\n",
    "    slice_start = max(0, word_count_before_entity - half_words_to_include)\n",
    "    slice_end = min(total_words, word_count_before_entity + len(entity_words) + half_words_to_include)\n",
    "    \n",
    "    # Adjust if the entity is towards the start or end of the sentence\n",
    "    if slice_end - slice_start < words_to_include:\n",
    "        if slice_start == 0:\n",
    "            slice_end = min(slice_start + words_to_include, total_words)\n",
    "        elif slice_end == total_words:\n",
    "            slice_start = max(0, slice_end - words_to_include)\n",
    "    \n",
    "    # Reconstruct the trimmed context\n",
    "    trimmed_context = ' '.join(words[slice_start:slice_end])\n",
    "    \n",
    "    return trimmed_context\n",
    "\n",
    "\n",
    "entity2 = \"lead vocalist\"\n",
    "context2 = \"Anita Auglend is the lead vocalist of the gothic-doom metal band.\"\n",
    "percentage2 = 60  # Adjust the percentage as needed\n",
    "trimmed_context2 = trim_context(entity2, context2, percentage2)\n",
    "print(trimmed_context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_text_to_json_v2(text_to_infer):\n",
    "    # Assume infer_years_for_dates, NERandREL, prepare_text_for_DOLCE_aligner, and serialize_doc_with_relations are defined elsewhere and operational\n",
    "    text_to_infer = infer_years_for_dates(text_to_infer)\n",
    "    doc = NERandREL(text=text_to_infer, config_path=\"./chained_classification/fewshot.cfg\", examples_path=\"./chained_classification/examples.jsonl\")\n",
    "\n",
    "    # Use serialize_doc_with_relations to get initial structure including relations\n",
    "    output_json = serialize_doc_with_relations(doc)\n",
    "\n",
    "    # Update the text in the output_json\n",
    "    output_json[\"text\"] = text_to_infer\n",
    "\n",
    "    # Process entities and match the new format\n",
    "    for index, entity in enumerate(doc.ents):\n",
    "        ent_dict = {\n",
    "            \"start_char\": entity.start_char,\n",
    "            \"end_char\": entity.end_char,\n",
    "            \"label\": entity.label_,\n",
    "            \"text\": entity.text\n",
    "        }\n",
    "        \n",
    "        # For CLASS entities, use DOLCEAligner to determine subclassOf information\n",
    "        if entity.label_ == \"CLASS\":\n",
    "            print(entity.text)\n",
    "            trimmed_context = trim_context(entity.text, text_to_infer, 50)\n",
    "            print(trimmed_context)\n",
    "            text = prepare_text_for_DOLCE_aligner(entity, trimmed_context)\n",
    "            runner = DOLCEAligner(config_path=\"./class_recognition/fewshot.cfg\", examples_path=\"./class_recognition/examples.jsonl\")\n",
    "            dolce_doc = runner.run(text)\n",
    "            filtered_categories = {label: score for label, score in dolce_doc.cats.items() if score > 0.0}\n",
    "            labels_with_positive_scores = list(filtered_categories.keys())\n",
    "\n",
    "            # Add the subclassOf information to the ent_dict\n",
    "            ent_dict[\"subClassOf\"] = labels_with_positive_scores if labels_with_positive_scores else [\"Unknown\"]\n",
    "\n",
    "            # Store class information separately with subclassOf details\n",
    "            formatted_text = entity.text.replace(\" \", \"_\")\n",
    "            key = f\"{formatted_text}_{index}\"\n",
    "            output_json['classes'][key] = {\"labels\": labels_with_positive_scores, \"class\": entity.text}\n",
    "\n",
    "        # Append entity information to the ents list in output_json\n",
    "        output_json[\"ents\"].append(ent_dict)\n",
    "\n",
    "    # Note: Relations are already included in output_json from the serialize_doc_with_relations call\n",
    "\n",
    "    return output_json\n",
    "\n",
    "\n",
    "\n",
    "# text_to_infer = \"Fut\\u016bh al-Buld\\u0101n is an Arabic book by Persian historian Ahmad Ibn Yahya al-Baladhuri. The work by which he is best known is the Kitab Futuh al-Buldan (\\\"Book of the Conquests of the Lands\\\"), edited by M. J. de Goeje as Liber expugnationis regionum (Leiden, 1870; Cairo, 1901).\"\n",
    "# processed_json = process_text_to_json_v2(text_to_infer)\n",
    "# with open(\"results/doc_data_1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(processed_json, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_substring_position(main_string, substring):\n",
    "    start_pos = main_string.find(substring)\n",
    "    if start_pos != -1:\n",
    "        end_pos = start_pos + len(substring)\n",
    "        return start_pos, end_pos\n",
    "    else:\n",
    "        return None, None  # Substring not found\n",
    "\n",
    "def read_and_check_json_file(file_path):\n",
    "    # Open and read the JSON file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    for e in data['ents']:\n",
    "        if e['text'] in data['text']:\n",
    "            start_real, end_real = find_substring_position(data['text'], e['text'])\n",
    "            start_j = e['start_char']\n",
    "            end_j = e['end_char']\n",
    "            print(start_real, start_j)\n",
    "            print(end_real, end_j)\n",
    "\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'examples_jsonl/adalgis.jsonl'\n",
    "\n",
    "# Run the script with the specified JSON file\n",
    "read_and_check_json_file(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Aaron Copland was an American composer, composition teacher, writer, and later in his career a conductor of his own and other American music.\"\n",
    "find_substring_position(text, \"American music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_and_return_data(input_file):\n",
    "    # Open and read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)  # Load the entire JSON file\n",
    "        \n",
    "        # Assume data is a list of objects, each with a \"sentence\" key\n",
    "        for index, item in enumerate(data):\n",
    "            # Extract the sentence text\n",
    "            sentence_text = item[\"sentence\"]\n",
    "            \n",
    "            # Process the text\n",
    "            processed_json = process_text_to_json_v2(sentence_text)\n",
    "            \n",
    "            # Construct the output filename\n",
    "            output_filename = f\"results-GPT-4/doc_data_{index}.json\"\n",
    "            \n",
    "            # Save the processed JSON to a file\n",
    "            with open(output_filename, \"w\", encoding='utf-8') as out_file:\n",
    "                json.dump(processed_json, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Processed and saved: {output_filename}\")\n",
    "\n",
    "# Example usage\n",
    "# Adjust the input file path to your actual JSON file path\n",
    "input_file_path = 'oke2_eval_dataset.json'\n",
    "parse_json_and_return_data(input_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
